{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('script')\n",
    "from script import dbconn\n",
    "pgconn = dbconn.db_connection_psycopg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fectching data from the postgreSql database and put the value on raw_df\n",
    "raw_df = dbconn.db_read_table_psycopg(pgconn,'xdr_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the top 10 handsets used by the customers\n",
    "top_10_handsets = raw_df['Handset Type'].value_counts().head(10)\n",
    "print(top_10_handsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the top 3 handset manufacturers\n",
    "top_3_manufacturers = raw_df['Handset Manufacturer'].value_counts().head(3)\n",
    "print(top_3_manufacturers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the top 5 handsets per top 3 handset manufacturer\n",
    "top_3_manufacturers = raw_df['Handset Manufacturer'].value_counts().head(3).index\n",
    "\n",
    "for manufacturer in top_3_manufacturers:\n",
    "    top_5_handsets = raw_df.loc[raw_df['Handset Manufacturer'] == manufacturer, 'Handset Type'].value_counts().head(5)\n",
    "    print(f\"Top 5 handsets for {manufacturer}:\")\n",
    "    print(top_5_handsets)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                   TASK 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of xDR sessions\n",
    "user_sessions = raw_df.groupby('MSISDN/Number')['Bearer Id'].count().reset_index()\n",
    "user_sessions.columns = ['MSISDN/Number', 'Number of xDR Sessions']\n",
    "print(user_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Session duration\n",
    "user_session_duration = raw_df.groupby('MSISDN/Number')['Dur. (ms)'].sum().reset_index()\n",
    "user_session_duration.columns = ['MSISDN/Number', 'Session Duration']\n",
    "print(user_session_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the total download (DL) and upload (UL) data\n",
    "user_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Total DL (Bytes)': 'sum',\n",
    "    'Total UL (Bytes)': 'sum'\n",
    "}).reset_index()\n",
    "user_data.columns = ['MSISDN/Number', 'Total DL Data', 'Total UL Data']\n",
    "print(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the total data volume (in Bytes) \n",
    "user_session_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Total UL (Bytes)': 'sum',\n",
    "    'Total DL (Bytes)': 'sum'\n",
    "}).reset_index()\n",
    "user_session_data['Total Data Volume'] = user_session_data['Total UL (Bytes)'] + user_session_data['Total DL (Bytes)']\n",
    "user_session_data = user_session_data[['MSISDN/Number', 'Total Data Volume']]\n",
    "print(user_session_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                     TASK 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percent of missing data\n",
    "\n",
    "def percent_missing(df):\n",
    "    # Calculate total number of cells in dataframe\n",
    "    totalCells = np.product(df.shape)\n",
    "\n",
    "    # Count number of missing values per column\n",
    "    missingCount = df.isnull().sum()\n",
    "\n",
    "    # Calculate total number of missing values\n",
    "    totalMissing = missingCount.sum()\n",
    "\n",
    "    # Calculate percentage of missing values\n",
    "    percentageMissing = (totalMissing / totalCells) * 100\n",
    "\n",
    "    print(\"The dataset contains\", round(percentageMissing, 2), \"%\", \"missing values.\")\n",
    "\n",
    "percent_missing(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify and replace outliers and missing values with column mean\n",
    "\n",
    "\n",
    "\n",
    "# Replace missing values with column mean\n",
    "raw_df.fillna(raw_df.mean(), inplace=True)\n",
    "\n",
    "# Identify and replace outliers with column mean\n",
    "num_columns = raw_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for col in num_columns:\n",
    "    z_scores = (raw_df[col] - raw_df[col].mean()) / raw_df[col].std()\n",
    "    outliers = (z_scores > 3) | (z_scores < -3)\n",
    "    raw_df[col][outliers] = raw_df[col].mean()\n",
    "\n",
    "# Verify missing values and outliers have been treated\n",
    "missing_values_after_treatment = raw_df.isnull().sum()\n",
    "print(\"Missing Values After Treatment:\\n\", missing_values_after_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values in each column\n",
    "missing_percent = (raw_df.isnull().sum() / len(raw_df)) * 100\n",
    "\n",
    "# Drop columns with more than 30% missing values\n",
    "columns_to_drop = missing_percent[missing_percent > 30].index\n",
    "df_clean = raw_df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# Print the shape of the cleaned DataFrame\n",
    "print(\"Shape of cleaned DataFrame:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = raw_df.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solving The rest of missing values\n",
    "def fix_missing_ffill(df, col):\n",
    "    df[col] = df[col].fillna(method='ffill')\n",
    "    return df[col]\n",
    "\n",
    "raw_df['Start'] = fix_missing_ffill(raw_df, 'Start')\n",
    "raw_df['End'] = fix_missing_ffill(raw_df, 'End')\n",
    "raw_df['Last Location Name'] = fix_missing_ffill(raw_df, 'Last Location Name')\n",
    "\n",
    "missing_values = raw_df.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate basic metrics\n",
    "metrics = raw_df.describe()\n",
    "mean = metrics.loc['mean']\n",
    "median = metrics.loc['50%']\n",
    "mode = raw_df.mode().iloc[0]\n",
    "minimum = metrics.loc['min']\n",
    "maximum = metrics.loc['max']\n",
    "std_deviation = metrics.loc['std']\n",
    "\n",
    "# Print the basic metrics\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"\\nMedian:\\n\", median)\n",
    "print(\"\\nMode:\\n\", mode)\n",
    "print(\"\\nMinimum:\\n\", minimum)\n",
    "print(\"\\nMaximum:\\n\", maximum)\n",
    "print(\"\\nStandard Deviation:\\n\", std_deviation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a Non-Graphical Univariate Analysis by computing dispersion parameters for each quantitative variable\n",
    "\n",
    "# Select quantitative variables in the dataset\n",
    "quantitative_vars = raw_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute dispersion parameters for each quantitative variable\n",
    "dispersion_parameters = quantitative_vars.agg(['mean', 'median', 'std', 'min', 'max', 'var'])\n",
    "\n",
    "# Print the dispersion parameters\n",
    "print(\"Dispersion Parameters:\\n\", dispersion_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a Graphical Univariate Analysis for each variable\n",
    "\n",
    "# Select variables in the dataset\n",
    "variables = raw_df.columns\n",
    "\n",
    "# Plotting options for each variable\n",
    "for variable in variables:\n",
    "    if raw_df[variable].dtype == 'int64' or raw_df[variable].dtype == 'float64':\n",
    "        # For numeric variables (continuous or discrete)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.histplot(data=raw_df, x=variable, kde=True)\n",
    "        plt.title(f'Distribution of {variable}')\n",
    "        plt.xlabel(variable)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "    else:\n",
    "        # For categorical variables\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.countplot(data=raw_df, x=variable)\n",
    "        plt.title(f'Count of {variable}')\n",
    "        plt.xlabel(variable)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Analysis\n",
    "variables = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "             'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',\n",
    "             'Other DL (Bytes)']\n",
    "\n",
    "# Subset the DataFrame with the selected variables\n",
    "subset_df = raw_df[variables]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = subset_df.corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensionality Reduction \n",
    "\n",
    "# Assuming you have a DataFrame 'raw_df' with the relevant variables\n",
    "variables = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "             'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',\n",
    "             'Other DL (Bytes)']\n",
    "\n",
    "# Subset the DataFrame with the selected variables\n",
    "subset_df = raw_df[variables]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(subset_df)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "pc_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Interpretation of the results\n",
    "print(\"Interpretation of PCA results:\")\n",
    "print(\"- The first principal component (PC1) explains\", round(explained_variance_ratio[0] * 100, 2), \"% of the variance in the data.\")\n",
    "print(\"- The second principal component (PC2) explains\", round(explained_variance_ratio[1] * 100, 2), \"% of the variance in the data.\")\n",
    "print(\"- PC1 captures the most significant patterns and trends in the data, such as overall data usage level.\")\n",
    "print(\"- PC2 captures additional variation that is orthogonal to PC1 and represents specific usage patterns or differences between the applications.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
