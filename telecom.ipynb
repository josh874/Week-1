{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('script')\n",
    "from script import dbconn\n",
    "pgconn = dbconn.db_connection_psycopg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fectching data from the postgreSql database and put the value on raw_df\n",
    "raw_df = dbconn.db_read_table_psycopg(pgconn,'xdr_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the top 10 handsets used by the customers\n",
    "top_10_handsets = raw_df['Handset Type'].value_counts().head(10)\n",
    "print(top_10_handsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the top 3 handset manufacturers\n",
    "top_3_manufacturers = raw_df['Handset Manufacturer'].value_counts().head(3)\n",
    "print(top_3_manufacturers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the top 5 handsets per top 3 handset manufacturer\n",
    "top_3_manufacturers = raw_df['Handset Manufacturer'].value_counts().head(3).index\n",
    "\n",
    "for manufacturer in top_3_manufacturers:\n",
    "    top_5_handsets = raw_df.loc[raw_df['Handset Manufacturer'] == manufacturer, 'Handset Type'].value_counts().head(5)\n",
    "    print(f\"Top 5 handsets for {manufacturer}:\")\n",
    "    print(top_5_handsets)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                   TASK 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of xDR sessions\n",
    "user_sessions = raw_df.groupby('MSISDN/Number')['Bearer Id'].count().reset_index()\n",
    "user_sessions.columns = ['MSISDN/Number', 'Number of xDR Sessions']\n",
    "print(user_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Session duration\n",
    "user_session_duration = raw_df.groupby('MSISDN/Number')['Dur. (ms)'].sum().reset_index()\n",
    "user_session_duration.columns = ['MSISDN/Number', 'Session Duration']\n",
    "print(user_session_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the total download (DL) and upload (UL) data\n",
    "user_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Total DL (Bytes)': 'sum',\n",
    "    'Total UL (Bytes)': 'sum'\n",
    "}).reset_index()\n",
    "user_data.columns = ['MSISDN/Number', 'Total DL Data', 'Total UL Data']\n",
    "print(user_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the total data volume (in Bytes) \n",
    "user_session_data = raw_df.groupby('MSISDN/Number').agg({\n",
    "    'Total UL (Bytes)': 'sum',\n",
    "    'Total DL (Bytes)': 'sum'\n",
    "}).reset_index()\n",
    "user_session_data['Total Data Volume'] = user_session_data['Total UL (Bytes)'] + user_session_data['Total DL (Bytes)']\n",
    "user_session_data = user_session_data[['MSISDN/Number', 'Total Data Volume']]\n",
    "print(user_session_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                     TASK 2.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percent of missing data\n",
    "\n",
    "def percent_missing(df):\n",
    "    # Calculate total number of cells in dataframe\n",
    "    totalCells = np.product(df.shape)\n",
    "\n",
    "    # Count number of missing values per column\n",
    "    missingCount = df.isnull().sum()\n",
    "\n",
    "    # Calculate total number of missing values\n",
    "    totalMissing = missingCount.sum()\n",
    "\n",
    "    # Calculate percentage of missing values\n",
    "    percentageMissing = (totalMissing / totalCells) * 100\n",
    "\n",
    "    print(\"The dataset contains\", round(percentageMissing, 2), \"%\", \"missing values.\")\n",
    "\n",
    "percent_missing(raw_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identify and replace outliers and missing values with column mean\n",
    "\n",
    "\n",
    "\n",
    "# Replace missing values with column mean\n",
    "raw_df.fillna(raw_df.mean(), inplace=True)\n",
    "\n",
    "# Identify and replace outliers with column mean\n",
    "num_columns = raw_df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "for col in num_columns:\n",
    "    z_scores = (raw_df[col] - raw_df[col].mean()) / raw_df[col].std()\n",
    "    outliers = (z_scores > 3) | (z_scores < -3)\n",
    "    raw_df[col][outliers] = raw_df[col].mean()\n",
    "\n",
    "# Verify missing values and outliers have been treated\n",
    "missing_values_after_treatment = raw_df.isnull().sum()\n",
    "print(\"Missing Values After Treatment:\\n\", missing_values_after_treatment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the percentage of missing values in each column\n",
    "missing_percent = (raw_df.isnull().sum() / len(raw_df)) * 100\n",
    "\n",
    "# Drop columns with more than 30% missing values\n",
    "columns_to_drop = missing_percent[missing_percent > 30].index\n",
    "df_clean = raw_df.drop(columns_to_drop, axis=1)\n",
    "\n",
    "# Print the shape of the cleaned DataFrame\n",
    "print(\"Shape of cleaned DataFrame:\", df_clean.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = raw_df.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Solving The rest of missing values\n",
    "def fix_missing_ffill(df, col):\n",
    "    df[col] = df[col].fillna(method='ffill')\n",
    "    return df[col]\n",
    "\n",
    "raw_df['Start'] = fix_missing_ffill(raw_df, 'Start')\n",
    "raw_df['End'] = fix_missing_ffill(raw_df, 'End')\n",
    "raw_df['Last Location Name'] = fix_missing_ffill(raw_df, 'Last Location Name')\n",
    "\n",
    "missing_values = raw_df.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate basic metrics\n",
    "metrics = raw_df.describe()\n",
    "mean = metrics.loc['mean']\n",
    "median = metrics.loc['50%']\n",
    "mode = raw_df.mode().iloc[0]\n",
    "minimum = metrics.loc['min']\n",
    "maximum = metrics.loc['max']\n",
    "std_deviation = metrics.loc['std']\n",
    "\n",
    "# Print the basic metrics\n",
    "print(\"Mean:\\n\", mean)\n",
    "print(\"\\nMedian:\\n\", median)\n",
    "print(\"\\nMode:\\n\", mode)\n",
    "print(\"\\nMinimum:\\n\", minimum)\n",
    "print(\"\\nMaximum:\\n\", maximum)\n",
    "print(\"\\nStandard Deviation:\\n\", std_deviation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a Non-Graphical Univariate Analysis by computing dispersion parameters for each quantitative variable\n",
    "\n",
    "# Select quantitative variables in the dataset\n",
    "quantitative_vars = raw_df.select_dtypes(include=[np.number])\n",
    "\n",
    "# Compute dispersion parameters for each quantitative variable\n",
    "dispersion_parameters = quantitative_vars.agg(['mean', 'median', 'std', 'min', 'max', 'var'])\n",
    "\n",
    "# Print the dispersion parameters\n",
    "print(\"Dispersion Parameters:\\n\", dispersion_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a Graphical Univariate Analysis for each variable\n",
    "\n",
    "# Select variables in the dataset\n",
    "variables = raw_df.columns\n",
    "\n",
    "# Plotting options for each variable\n",
    "for variable in variables:\n",
    "    if raw_df[variable].dtype == 'int64' or raw_df[variable].dtype == 'float64':\n",
    "        # For numeric variables (continuous or discrete)\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.histplot(data=raw_df, x=variable, kde=True)\n",
    "        plt.title(f'Distribution of {variable}')\n",
    "        plt.xlabel(variable)\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "    else:\n",
    "        # For categorical variables\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.countplot(data=raw_df, x=variable)\n",
    "        plt.title(f'Count of {variable}')\n",
    "        plt.xlabel(variable)\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Correlation Analysis\n",
    "variables = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "             'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',\n",
    "             'Other DL (Bytes)']\n",
    "\n",
    "# Subset the DataFrame with the selected variables\n",
    "subset_df = raw_df[variables]\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = subset_df.corr()\n",
    "\n",
    "# Print the correlation matrix\n",
    "print(\"Correlation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimensionality Reduction \n",
    "\n",
    "# Assuming you have a DataFrame 'raw_df' with the relevant variables\n",
    "variables = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "             'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',\n",
    "             'Other DL (Bytes)']\n",
    "\n",
    "# Subset the DataFrame with the selected variables\n",
    "subset_df = raw_df[variables]\n",
    "\n",
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(subset_df)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA(n_components=2)\n",
    "principal_components = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "pc_df = pd.DataFrame(data=principal_components, columns=['PC1', 'PC2'])\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Interpretation of the results\n",
    "print(\"Interpretation of PCA results:\")\n",
    "print(\"- The first principal component (PC1) explains\", round(explained_variance_ratio[0] * 100, 2), \"% of the variance in the data.\")\n",
    "print(\"- The second principal component (PC2) explains\", round(explained_variance_ratio[1] * 100, 2), \"% of the variance in the data.\")\n",
    "print(\"- PC1 captures the most significant patterns and trends in the data, such as overall data usage level.\")\n",
    "print(\"- PC2 captures additional variation that is orthogonal to PC1 and represents specific usage patterns or differences between the applications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "      Task 3 - User Engagement analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#top 10 customers per engagement metric \n",
    "\n",
    "# Engagement metrics\n",
    "engagement_metrics = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "                      'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',\n",
    "                      'Other DL (Bytes)']\n",
    "\n",
    "# Group by 'MSISDN/Number' and calculate the sum of each engagement metric\n",
    "grouped_df = raw_df.groupby('MSISDN/Number')[engagement_metrics].sum()\n",
    "\n",
    "# Calculate the total engagement metric for each customer\n",
    "grouped_df['Total Engagement'] = grouped_df.sum(axis=1)\n",
    "\n",
    "# Report the top 10 customers for each engagement metric\n",
    "for metric in engagement_metrics:\n",
    "    top_10_customers = grouped_df.nlargest(10, metric)\n",
    "    print(f\"Top 10 customers for {metric}:\")\n",
    "    print(top_10_customers)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize each engagement metric and run a k-means (k=3) to classify customers in three groups of engagement. \n",
    "# Engagement metrics\n",
    "engagement_metrics = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "                      'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',\n",
    "                      'Other DL (Bytes)']\n",
    "\n",
    "# Select the engagement metrics from the DataFrame\n",
    "engagement_data = raw_df[engagement_metrics]\n",
    "\n",
    "# Normalize the engagement data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(engagement_data)\n",
    "\n",
    "# Perform k-means clustering with k=3\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "kmeans.fit(normalized_data)\n",
    "\n",
    "# Assign the cluster labels to the original DataFrame\n",
    "raw_df['Engagement Cluster'] = kmeans.labels_\n",
    "\n",
    "# Print the number of customers in each engagement cluster\n",
    "print(raw_df['Engagement Cluster'].value_counts())\n",
    "\n",
    "# Alternatively, you can group the data by the engagement cluster and calculate statistics\n",
    "cluster_stats = raw_df.groupby('Engagement Cluster')[engagement_metrics].mean()\n",
    "print(cluster_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the minimum, maximum, average & total non-normalized metrics for each cluster\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "# Engagement metrics\n",
    "engagement_metrics = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "                      'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',\n",
    "                      'Other DL (Bytes)']\n",
    "\n",
    "# Select the engagement metrics from the DataFrame\n",
    "engagement_data = raw_df[engagement_metrics]\n",
    "\n",
    "# Normalize the engagement data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(engagement_data)\n",
    "\n",
    "# Perform k-means clustering with k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(normalized_data)\n",
    "\n",
    "# Assign the cluster labels to the original DataFrame\n",
    "raw_df['Engagement Cluster'] = kmeans.labels_\n",
    "\n",
    "# Compute non-normalized metrics for each cluster\n",
    "cluster_stats = raw_df.groupby('Engagement Cluster')[engagement_metrics].agg(['min', 'max', 'mean', 'sum'])\n",
    "\n",
    "# Visualize the results\n",
    "cluster_stats.plot(kind='bar', figsize=(12, 6))\n",
    "plt.xlabel('Engagement Cluster')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Metrics for Each Engagement Cluster')\n",
    "plt.legend(['Min', 'Max', 'Mean', 'Sum'])\n",
    "plt.show()\n",
    "\n",
    "# Print the cluster statistics\n",
    "print(cluster_stats)\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Assuming 'raw_df' is the DataFrame containing the metrics\n",
    "\n",
    "# Engagement metrics\n",
    "engagement_metrics = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "                      'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',\n",
    "                      'Other DL (Bytes)']\n",
    "\n",
    "# Select the engagement metrics from the DataFrame\n",
    "engagement_data = raw_df[engagement_metrics]\n",
    "\n",
    "# Normalize the engagement data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(engagement_data)\n",
    "\n",
    "# Perform k-means clustering with k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(normalized_data)\n",
    "\n",
    "# Assign the cluster labels to the original DataFrame\n",
    "raw_df['Engagement Cluster'] = kmeans.labels_\n",
    "\n",
    "# Compute non-normalized metrics for each cluster\n",
    "cluster_stats = raw_df.groupby('Engagement Cluster')[engagement_metrics].agg(['min', 'max', 'mean', 'sum'])\n",
    "\n",
    "# Visualize the results\n",
    "cluster_stats.plot(kind='bar', figsize=(12, 6))\n",
    "plt.xlabel('Engagement Cluster')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Metrics for Each Engagement Cluster')\n",
    "plt.legend(['Min', 'Max', 'Mean', 'Sum'])\n",
    "plt.show()\n",
    "\n",
    "# Print the cluster statistics\n",
    "print(cluster_stats)\n",
    "\n",
    "# Engagement metrics\n",
    "engagement_metrics = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "                      'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',\n",
    "                      'Other DL (Bytes)']\n",
    "\n",
    "# Select the engagement metrics from the DataFrame\n",
    "engagement_data = raw_df[engagement_metrics]\n",
    "\n",
    "# Normalize the engagement data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(engagement_data)\n",
    "\n",
    "# Perform k-means clustering with k=3\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "kmeans.fit(normalized_data)\n",
    "\n",
    "# Assign the cluster labels to the original DataFrame\n",
    "raw_df['Engagement Cluster'] = kmeans.labels_\n",
    "\n",
    "# Compute non-normalized metrics for each cluster\n",
    "cluster_stats = raw_df.groupby('Engagement Cluster')[engagement_metrics].agg(['min', 'max', 'mean', 'sum'])\n",
    "\n",
    "# Visualize the results\n",
    "cluster_stats.plot(kind='bar', figsize=(12, 6))\n",
    "plt.xlabel('Engagement Cluster')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Metrics for Each Engagement Cluster')\n",
    "plt.legend(['Min', 'Max', 'Mean', 'Sum'])\n",
    "plt.show()\n",
    "\n",
    "# Print the cluster statistics\n",
    "print(cluster_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate user total traffic per application and derive the top 10 most engaged users per application\n",
    "\n",
    "\n",
    "# Application columns\n",
    "application_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "                       'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',\n",
    "                       'Other DL (Bytes)']\n",
    "\n",
    "# Calculate total traffic per application for each user\n",
    "user_traffic = raw_df.groupby('MSISDN/Number')[application_columns].sum()\n",
    "\n",
    "# Derive the top 10 most engaged users per application\n",
    "top_10_users_per_app = pd.DataFrame()\n",
    "for column in application_columns:\n",
    "    top_10_users = user_traffic.nlargest(10, column)\n",
    "    top_10_users_per_app[column] = top_10_users.index\n",
    "\n",
    "# Print the top 10 most engaged users per application\n",
    "print(\"Top 10 most engaged users per application:\")\n",
    "print(top_10_users_per_app)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the top 3 most used applications using appropriate charts. \n",
    "\n",
    "\n",
    "# Application columns\n",
    "application_columns = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "                       'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',\n",
    "                       'Other DL (Bytes)']\n",
    "\n",
    "# Calculate total traffic per application\n",
    "total_traffic = raw_df[application_columns].sum()\n",
    "\n",
    "# Get the top 3 most used applications\n",
    "top_3_applications = total_traffic.nlargest(3)\n",
    "\n",
    "# Plot the top 3 most used applications\n",
    "plt.bar(top_3_applications.index, top_3_applications.values)\n",
    "plt.xlabel('Application')\n",
    "plt.ylabel('Total Traffic')\n",
    "plt.title('Top 3 Most Used Applications')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#determine the optimized value of k for grouping users into engagement clusters using the k-means clustering algorithm and the elbow method\n",
    "\n",
    "\n",
    "# Engagement metrics\n",
    "engagement_metrics = ['Social Media DL (Bytes)', 'Google DL (Bytes)', 'Email DL (Bytes)',\n",
    "                      'Youtube DL (Bytes)', 'Netflix DL (Bytes)', 'Gaming DL (Bytes)',\n",
    "                      'Other DL (Bytes)']\n",
    "\n",
    "# Select the engagement metrics from the DataFrame\n",
    "engagement_data = raw_df[engagement_metrics]\n",
    "\n",
    "# Normalize the engagement data using Min-Max scaling\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(engagement_data)\n",
    "\n",
    "# Perform k-means clustering for a range of k values\n",
    "k_values = range(1, 11)  # Try k values from 1 to 10\n",
    "inertia_values = []  # List to store the inertia (sum of squared distances) for each k value\n",
    "\n",
    "for k in k_values:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    kmeans.fit(normalized_data)\n",
    "    inertia_values.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow curve\n",
    "plt.plot(k_values, inertia_values, marker='o')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.title('Elbow Curve')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TASK : 4 -  Experience Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the average TCP retransmission per customer\n",
    "customer_avg_retransmission = raw_df.groupby('MSISDN/Number')['TCP DL Retrans. Vol (Bytes)'].mean()\n",
    "\n",
    "# Print the aggregated information\n",
    "print(customer_avg_retransmission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the average RTT per customer\n",
    "customer_avg_rtt = raw_df.groupby('MSISDN/Number')['Avg RTT DL (ms)'].mean()\n",
    "\n",
    "# Print the aggregated information\n",
    "print(customer_avg_rtt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the handset type per customer\n",
    "customer_handset_type = raw_df.groupby('MSISDN/Number')['Handset Type'].first()\n",
    "\n",
    "# Print the aggregated information\n",
    "print(customer_handset_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate the average throughput per customer\n",
    "customer_avg_throughput = raw_df.groupby('MSISDN/Number')['Avg Bearer TP DL (kbps)'].mean()\n",
    "\n",
    "# Print the aggregated information\n",
    "print(customer_avg_throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the top, bottom, and most frequent TCP values \n",
    "# Compute the top 10 TCP values\n",
    "top_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].nlargest(10)\n",
    "print(\"Top 10 TCP Values:\")\n",
    "print(top_tcp_values)\n",
    "\n",
    "# Compute the bottom 10 TCP values\n",
    "bottom_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].nsmallest(10)\n",
    "print(\"\\nBottom 10 TCP Values:\")\n",
    "print(bottom_tcp_values)\n",
    "\n",
    "# Compute the most frequent TCP values\n",
    "most_frequent_tcp_values = raw_df['TCP DL Retrans. Vol (Bytes)'].value_counts().head(10)\n",
    "print(\"\\nMost Frequent TCP Values:\")\n",
    "print(most_frequent_tcp_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the top, bottom, and most frequent RTT values\n",
    "# Compute the top 10 RTT values\n",
    "top_rtt_values = raw_df['Avg RTT DL (ms)'].nlargest(10)\n",
    "print(\"Top 10 RTT Values:\")\n",
    "print(top_rtt_values)\n",
    "\n",
    "# Compute the bottom 10 RTT values\n",
    "bottom_rtt_values = raw_df['Avg RTT DL (ms)'].nsmallest(10)\n",
    "print(\"\\nBottom 10 RTT Values:\")\n",
    "print(bottom_rtt_values)\n",
    "\n",
    "# Compute the most frequent RTT values\n",
    "most_frequent_rtt_values = raw_df['Avg RTT DL (ms)'].value_counts().head(10)\n",
    "print(\"\\nMost Frequent RTT Values:\")\n",
    "print(most_frequent_rtt_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the top, bottom, and most frequent throughput values\n",
    "# Compute the top 10 throughput values\n",
    "top_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].nlargest(10)\n",
    "print(\"Top 10 Throughput Values:\")\n",
    "print(top_throughput_values)\n",
    "\n",
    "# Compute the bottom 10 throughput values\n",
    "bottom_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].nsmallest(10)\n",
    "print(\"\\nBottom 10 Throughput Values:\")\n",
    "print(bottom_throughput_values)\n",
    "\n",
    "# Compute the most frequent throughput values\n",
    "most_frequent_throughput_values = raw_df['Avg Bearer TP DL (kbps)'].value_counts().head(10)\n",
    "print(\"\\nMost Frequent Throughput Values:\")\n",
    "print(most_frequent_throughput_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the distribution of the average throughput per handset type\n",
    "\n",
    "# Group the data by handset type and calculate the mean throughput\n",
    "grouped_df = raw_df.groupby('Handset Type')['Avg Bearer TP DL (kbps)'].mean()\n",
    "\n",
    "# Report the distribution of the average throughput per handset type\n",
    "print(\"Distribution of Average Throughput per Handset Type:\")\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the average TCP retransmission view per handset type\n",
    "# Group the data by handset type and calculate the mean TCP retransmission view\n",
    "grouped_df = raw_df.groupby('Handset Type')['TCP DL Retrans. Vol (Bytes)'].mean()\n",
    "\n",
    "# Report the average TCP retransmission view per handset type\n",
    "print(\"Average TCP Retransmission View per Handset Type:\")\n",
    "print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A k-means clustering (where k = 3) to segment users into groups\n",
    "\n",
    "# Select the relevant experience metrics for clustering\n",
    "selected_columns = ['Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',\n",
    "                    'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)']\n",
    "\n",
    "# Create a new DataFrame with the selected columns\n",
    "metrics_df = raw_df[selected_columns]\n",
    "\n",
    "# Perform feature scaling\n",
    "scaler = StandardScaler()\n",
    "scaled_metrics = scaler.fit_transform(metrics_df)\n",
    "\n",
    "# Set the number of clusters (k)\n",
    "k = 3\n",
    "\n",
    "# Apply k-means clustering\n",
    "kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "kmeans.fit(scaled_metrics)\n",
    "\n",
    "# Get the cluster labels assigned to each data point\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Add the cluster labels to the original DataFrame\n",
    "raw_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Define cluster descriptions based on your understanding of the data\n",
    "cluster_descriptions = {\n",
    "    0: \"Cluster with high RTT, high average bearer throughput, and high TCP retransmission\",\n",
    "    1: \"Cluster with moderate RTT, average bearer throughput, and TCP retransmission\",\n",
    "    2: \"Cluster with low RTT, low average bearer throughput, and low TCP retransmission\"\n",
    "}\n",
    "\n",
    "# Print the count of users in each cluster\n",
    "print(\"Cluster Counts:\")\n",
    "print(raw_df['Cluster'].value_counts())\n",
    "\n",
    "# Print the description of each cluster\n",
    "print(\"\\nCluster Descriptions:\")\n",
    "for cluster, description in cluster_descriptions.items():\n",
    "    print(f\"Cluster {cluster}: {description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5 - Satisfaction Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Engagement score to each user.\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "# Select the relevant columns for calculating the engagement score\n",
    "engagement_df = raw_df[['Bearer Id', 'Dur. (ms)', 'Activity Duration DL (ms)', 'Activity Duration UL (ms)']]\n",
    "\n",
    "# Placeholder DataFrame for the less engaged cluster\n",
    "less_engaged_cluster_df = pd.DataFrame({\n",
    "    'Dur. (ms)': [100, 200, 300], \n",
    "    'Activity Duration DL (ms)': [150, 250, 350],  \n",
    "    'Activity Duration UL (ms)': [120, 220, 320]  \n",
    "})\n",
    "\n",
    "# Create an empty list to store the engagement scores\n",
    "engagement_scores = []\n",
    "\n",
    "# Iterate over each user in the engagement_df DataFrame\n",
    "for _, user_data in engagement_df.iterrows():\n",
    "    # Extract the relevant data points for the user\n",
    "    user_data_points = user_data[['Dur. (ms)', 'Activity Duration DL (ms)', 'Activity Duration UL (ms)']].values\n",
    "    \n",
    "    # Calculate the Euclidean distance between the user data points and each data point in the less engaged cluster\n",
    "    distances = []\n",
    "    for _, cluster_data in less_engaged_cluster_df.iterrows():\n",
    "        cluster_data_points = cluster_data.values\n",
    "        euclidean_distance = distance.euclidean(user_data_points, cluster_data_points)\n",
    "        distances.append(euclidean_distance)\n",
    "    \n",
    "    # Get the minimum distance as the engagement score\n",
    "    engagement_score = min(distances)\n",
    "    \n",
    "    # Append the engagement score to the list\n",
    "    engagement_scores.append(engagement_score)\n",
    "    \n",
    "    # Print the engagement score for the user\n",
    "    print(f\"User: {user_data['Bearer Id']}, Engagement Score: {engagement_score}\")\n",
    "\n",
    "# Add the engagement scores to the raw_df DataFrame\n",
    "raw_df['Engagement Score'] = engagement_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#experience score to each user\n",
    "\n",
    "# Select the relevant columns for calculating the experience score\n",
    "experience_df = raw_df[['Bearer Id', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',\n",
    "                       'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)', 'Email DL (Bytes)', 'Email UL (Bytes)',\n",
    "                       'Youtube DL (Bytes)', 'Youtube UL (Bytes)', 'Netflix DL (Bytes)', 'Netflix UL (Bytes)',\n",
    "                       'Gaming DL (Bytes)', 'Gaming UL (Bytes)', 'Other DL (Bytes)']]\n",
    "\n",
    "# Placeholder DataFrame for the worst experience cluster\n",
    "worst_experience_cluster_df = pd.DataFrame({\n",
    "    'Avg RTT DL (ms)': [100, 200, 300],  \n",
    "    'Avg RTT UL (ms)': [150, 250, 350],  \n",
    "    'Avg Bearer TP DL (kbps)': [50, 100, 150],  \n",
    "    'Avg Bearer TP UL (kbps)': [80, 120, 160],  \n",
    "    'TCP DL Retrans. Vol (Bytes)': [100000, 200000, 300000],  \n",
    "    'TCP UL Retrans. Vol (Bytes)': [120000, 220000, 320000],  \n",
    "    'Email DL (Bytes)': [500000, 600000, 700000],  \n",
    "    'Email UL (Bytes)': [550000, 650000, 750000], \n",
    "    'Youtube DL (Bytes)': [200000000, 300000000, 400000000],  \n",
    "    'Youtube UL (Bytes)': [250000000, 350000000, 450000000],  \n",
    "    'Netflix DL (Bytes)': [150000000, 250000000, 350000000],  \n",
    "    'Netflix UL (Bytes)': [180000000, 280000000, 380000000],  \n",
    "    'Gaming DL (Bytes)': [1000000000, 2000000000, 3000000000],  \n",
    "    'Gaming UL (Bytes)': [1200000000, 2200000000, 3200000000],  \n",
    "    'Other DL (Bytes)': [800000000, 900000000, 1000000000]  \n",
    "})\n",
    "\n",
    "# Create an empty list to store the experience scores\n",
    "experience_scores = []\n",
    "\n",
    "# Iterate over each user in the experience_df DataFrame\n",
    "for _, user_data in experience_df.iterrows():\n",
    "    # Extract the relevant data points for the user\n",
    "    user_data_points = user_data[['Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',\n",
    "                                  'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)', 'Email DL (Bytes)', 'Email UL (Bytes)',\n",
    "                                  'Youtube DL (Bytes)', 'Youtube UL (Bytes)', 'Netflix DL (Bytes)', 'Netflix UL (Bytes)',\n",
    "                                  'Gaming DL (Bytes)', 'Gaming UL (Bytes)', 'Other DL (Bytes)']].values\n",
    "    \n",
    "    # Calculate the Euclidean distance between the user data points and the worst experience cluster\n",
    "    distances = []\n",
    "    for _, cluster_data in worst_experience_cluster_df.iterrows():\n",
    "        cluster_data_points = cluster_data.values\n",
    "        euclidean_distance = distance.euclidean(user_data_points, cluster_data_points)\n",
    "        distances.append(euclidean_distance)\n",
    "    \n",
    "    # Get the minimum distance as the experience score\n",
    "    experience_score = min(distances)\n",
    "    \n",
    "    # Append the experience score to the list\n",
    "    experience_scores.append(experience_score)\n",
    "\n",
    "# Add the experience scores to the experience_df DataFrame\n",
    "experience_df['Experience Score'] = experience_scores\n",
    "\n",
    "# Print the experience scores for each user\n",
    "print(experience_df[['Bearer Id', 'Experience Score']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The top 10 satisfied customers \n",
    "\n",
    "# Select the relevant columns for calculating the satisfaction score\n",
    "satisfaction_df = raw_df[['Bearer Id', 'Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',\n",
    "                       'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)', 'Email DL (Bytes)', 'Email UL (Bytes)',\n",
    "                       'Youtube DL (Bytes)', 'Youtube UL (Bytes)', 'Netflix DL (Bytes)', 'Netflix UL (Bytes)',\n",
    "                       'Gaming DL (Bytes)', 'Gaming UL (Bytes)', 'Other DL (Bytes)']]\n",
    "\n",
    "# Placeholder DataFrame for the worst experience cluster\n",
    "worst_experience_cluster_df = pd.DataFrame({\n",
    "    'Avg RTT DL (ms)': [100, 200, 300],  \n",
    "    'Avg RTT UL (ms)': [150, 250, 350],  \n",
    "    'Avg Bearer TP DL (kbps)': [50, 100, 150],  \n",
    "    'Avg Bearer TP UL (kbps)': [80, 120, 160],  \n",
    "    'TCP DL Retrans. Vol (Bytes)': [100000, 200000, 300000], \n",
    "    'TCP UL Retrans. Vol (Bytes)': [120000, 220000, 320000],  \n",
    "    'Email DL (Bytes)': [500000, 600000, 700000],  \n",
    "    'Email UL (Bytes)': [550000, 650000, 750000],  \n",
    "    'Youtube DL (Bytes)': [200000000, 300000000, 400000000],  \n",
    "    'Youtube UL (Bytes)': [250000000, 350000000, 450000000],  \n",
    "    'Netflix DL (Bytes)': [150000000, 250000000, 350000000],  \n",
    "    'Netflix UL (Bytes)': [180000000, 280000000, 380000000],  \n",
    "    'Gaming DL (Bytes)': [1000000000, 2000000000, 3000000000], \n",
    "    'Gaming UL (Bytes)': [1200000000, 2200000000, 3200000000], \n",
    "    'Other DL (Bytes)': [800000000, 900000000, 1000000000]  \n",
    "})\n",
    "\n",
    "# Create an empty list to store the satisfaction scores\n",
    "satisfaction_scores = []\n",
    "\n",
    "# Iterate over each user in the satisfaction_df DataFrame\n",
    "for _, user_data in satisfaction_df.iterrows():\n",
    "    # Extract the relevant data points for the user\n",
    "    user_data_points = user_data[['Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',\n",
    "                                  'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)', 'Email DL (Bytes)', 'Email UL (Bytes)',\n",
    "                                  'Youtube DL (Bytes)', 'Youtube UL (Bytes)', 'Netflix DL (Bytes)', 'Netflix UL (Bytes)',\n",
    "                                  'Gaming DL (Bytes)', 'Gaming UL (Bytes)', 'Other DL (Bytes)']].values\n",
    "    \n",
    "    # Calculate the Euclidean distance between the user data points and the worst experience cluster\n",
    "    distances = []\n",
    "    for _, cluster_data in worst_experience_cluster_df.iterrows():\n",
    "        cluster_data_points = cluster_data.values\n",
    "        euclidean_distance = distance.euclidean(user_data_points, cluster_data_points)\n",
    "        distances.append(euclidean_distance)\n",
    "    \n",
    "    # Get the minimum distance as the experience score\n",
    "    experience_score = min(distances)\n",
    "    \n",
    "    # Calculate the engagement score (You need to replace this with your actual calculation)\n",
    "    engagement_score = 0  # Replace with your calculation\n",
    "    \n",
    "\n",
    "    # Calculate the satisfaction score as the average of experience and engagement scores\n",
    "    satisfaction_score = (experience_score + engagement_score) / 2\n",
    "    \n",
    "    # Append the satisfaction score to the list\n",
    "    satisfaction_scores.append(satisfaction_score)\n",
    "\n",
    "# Create a new column 'Satisfaction Score' in the satisfaction_df DataFrame\n",
    "satisfaction_df['Satisfaction Score'] = satisfaction_scores\n",
    "\n",
    "# Sort the DataFrame based on the 'Satisfaction Score' column in descending order\n",
    "sorted_df = satisfaction_df.sort_values(by='Satisfaction Score', ascending=False)\n",
    "\n",
    "# Get the top 10 satisfied customers\n",
    "top_10_satisfied_customers = sorted_df.head(10)\n",
    "\n",
    "# Print the top 10 satisfied customers\n",
    "print(top_10_satisfied_customers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regression model to predict the satisfaction score of a customer. \n",
    "\n",
    "\n",
    "# Select the relevant columns for calculating the satisfaction score\n",
    "satisfaction_df = raw_df[['Avg RTT DL (ms)', 'Avg RTT UL (ms)', 'Avg Bearer TP DL (kbps)', 'Avg Bearer TP UL (kbps)',\n",
    "                       'TCP DL Retrans. Vol (Bytes)', 'TCP UL Retrans. Vol (Bytes)', 'Email DL (Bytes)', 'Email UL (Bytes)',\n",
    "                       'Youtube DL (Bytes)', 'Youtube UL (Bytes)', 'Netflix DL (Bytes)', 'Netflix UL (Bytes)',\n",
    "                       'Gaming DL (Bytes)', 'Gaming UL (Bytes)', 'Other DL (Bytes)', 'Satisfaction Score']]\n",
    "\n",
    "# Split the data into features (input variables) and target variable (satisfaction score)\n",
    "features = satisfaction_df.drop('Satisfaction Score', axis=1)\n",
    "target = satisfaction_df['Satisfaction Score']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest Regressor model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using mean squared error (MSE)\n",
    "mse = mean_squared_error(y_test, predictions)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "# Example: Predict the satisfaction score for a new customer\n",
    "new_customer = pd.DataFrame({\n",
    "    'Avg RTT DL (ms)': [100],  # Example values, replace with your actual data\n",
    "    'Avg RTT UL (ms)': [150],  # Example values, replace with your actual data\n",
    "    'Avg Bearer TP DL (kbps)': [50],  # Example values, replace with your actual data\n",
    "    'Avg Bearer TP UL (kbps)': [80],  # Example values, replace with your actual data\n",
    "    'TCP DL Retrans. Vol (Bytes)': [100000],  # Example values, replace with your actual data\n",
    "    'TCP UL Retrans. Vol (Bytes)': [120000],  # Example values, replace with your actual data\n",
    "    'Email DL (Bytes)': [500000],  # Example values, replace with your actual data\n",
    "    'Email UL (Bytes)': [550000],  # Example values, replace with your actual data\n",
    "    'Youtube DL (Bytes)': [200000000],  # Example values, replace with your actual data\n",
    "    'Youtube UL (Bytes)': [250000000],  # Example values, replace with your actual data\n",
    "    'Netflix DL (Bytes)': [150000000],  # Example values, replace with your actual data\n",
    "    'Netflix UL (Bytes)': [180000000],  # Example values, replace with your actual data\n",
    "    'Gaming DL (Bytes)': [1000000000],  # Example values, replace with your actual data\n",
    "    'Gaming UL (Bytes)': [1200000000],  # Example values, replace with your actual data\n",
    "    'Other DL (Bytes)': [800000000]  # Example values, replace with your actual data\n",
    "})\n",
    "\n",
    "predicted_score = model.predict(new_customer)\n",
    "print(f\"Predicted Satisfaction Score: {predicted_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means (k=2) on the engagement & the experience score\n",
    "\n",
    "# Select the columns for engagement and experience scores\n",
    "engagement_experience_df = raw_df[['Engagement Score', 'Experience Score']]\n",
    "\n",
    "# Perform k-means clustering with k=2\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(engagement_experience_df)\n",
    "\n",
    "# Get the cluster labels assigned by the k-means algorithm\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Add the cluster labels as a new column in the DataFrame\n",
    "engagement_experience_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.scatter(engagement_experience_df['Engagement Score'], engagement_experience_df['Experience Score'], c=cluster_labels)\n",
    "plt.xlabel('Engagement Score')\n",
    "plt.ylabel('Experience Score')\n",
    "plt.title('K-means Clustering (k=2)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average satisfaction & experience score per cluster. \n",
    "\n",
    "\n",
    "# Select the columns for engagement and experience scores\n",
    "engagement_experience_df = raw_df[['Engagement Score', 'Experience Score']]\n",
    "\n",
    "# Perform k-means clustering with k=2\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(engagement_experience_df)\n",
    "\n",
    "# Get the cluster labels assigned by the k-means algorithm\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Add the cluster labels as a new column in the DataFrame\n",
    "engagement_experience_df['Cluster'] = cluster_labels\n",
    "\n",
    "# Calculate the average satisfaction and experience scores per cluster\n",
    "cluster_scores = engagement_experience_df.groupby('Cluster').mean()\n",
    "\n",
    "print(cluster_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
